{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h9464I-uxLiw"
   },
   "source": [
    "# Custom implementation of Tf-Idf vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OnV82tg1xLi0"
   },
   "source": [
    "### Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bUsYm9wjxLi1"
   },
   "outputs": [],
   "source": [
    "corpus = [\n",
    "     'this is the first document',\n",
    "     'this document is the second document',\n",
    "     'and this is the third one',\n",
    "     'is this the first document',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eLwmFZfKxLi4"
   },
   "source": [
    "### SkLearn Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Np4dfQOkxLi4"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(corpus)\n",
    "skl_output = vectorizer.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-7Om8YpYxLi6",
    "outputId": "0a3bd0f5-4424-4400-944f-4482a80bd799"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dTKplK96xLi-",
    "outputId": "53722fa2-6756-4aa0-f179-37b578bb6890"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.91629073 1.22314355 1.51082562 1.         1.91629073 1.91629073\n",
      " 1.         1.91629073 1.        ]\n"
     ]
    }
   ],
   "source": [
    "# idf values of vocab\n",
    "print(vectorizer.idf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-CTiWHygxLjA",
    "outputId": "8d5a9cde-2c29-4afe-f7b4-1547e88dba4f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 9)"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of sklearn tfidf vectorizer output after applying transform method.\n",
    "skl_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bDKEpbA-xLjD",
    "outputId": "87dafd65-5313-443f-8c6e-1b05cc8c2543"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 8)\t0.38408524091481483\n",
      "  (0, 6)\t0.38408524091481483\n",
      "  (0, 3)\t0.38408524091481483\n",
      "  (0, 2)\t0.5802858236844359\n",
      "  (0, 1)\t0.46979138557992045\n"
     ]
    }
   ],
   "source": [
    "# sklearn tfidf values for first line of the above corpus.\n",
    "# Here the output is a sparse matrix\n",
    "\n",
    "print(skl_output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3QWo34hexLjF",
    "outputId": "cdc04e08-989f-4bdc-dd7f-f1c82a9f90be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]]\n"
     ]
    }
   ],
   "source": [
    "# sklearn tfidf values for first line of the above corpus.\n",
    "# To understand the output better, here we are converting the sparse output matrix to dense matrix and printing it.\n",
    "# this output is normalized using L2 normalization. sklearn does this by default.\n",
    "\n",
    "print(skl_output[0].toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qfIwx5LzxLjI"
   },
   "source": [
    "### Your custom implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HjuCcJwXxLjJ",
    "outputId": "36e8c360-a2b4-4aa1-cae1-3f8b8facc4b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab = {'and': (0, 1.916290731874155), 'document': (1, 1.2231435513142097), 'first': (2, 1.5108256237659907), 'is': (3, 1.0), 'one': (4, 1.916290731874155), 'second': (5, 1.916290731874155), 'the': (6, 1.0), 'third': (7, 1.916290731874155), 'this': (8, 1.0)}\n"
     ]
    }
   ],
   "source": [
    "# Write your code here.\n",
    "# Try not to hardcode any values.\n",
    "# Make sure its well documented and readble with appropriate comments.\n",
    "\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix\n",
    "import math\n",
    "import operator\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy\n",
    "\n",
    "#Fit Method\n",
    "def fit(corpus):    \n",
    "  unique_words = set()\n",
    "  if isinstance(corpus, (list,)):\n",
    "    for row in corpus: \n",
    "      for word in row.split(\" \"): \n",
    "        if len(word) < 2:\n",
    "          continue\n",
    "        unique_words.add(word)\n",
    "    unique_words = sorted(list(unique_words))\n",
    "    vocab1= {j:i for i,j in enumerate(unique_words)} \n",
    "    p={}\n",
    "    for x in vocab1:\n",
    "      q=0\n",
    "      for y in corpus:\n",
    "        if x in y.split(\" \"):            \n",
    "          q+=1      \n",
    "      p[x] = 1 + math.log((len(corpus)+1)/(q+1))\n",
    "      vocab_sorted=sorted(p.items(),key=lambda x:x[0],reverse=False)\n",
    "      vocab={j[0]:(i,j[1]) for i,j in enumerate(vocab_sorted)} \n",
    "    return vocab       \n",
    "  else:\n",
    "    print(\"you need to pass list of sentance\")\n",
    "\n",
    "corpus = [\n",
    "     'this is the first document',  \n",
    "     'this document is the second document',\n",
    "     'and this is the third one',\n",
    "     'is this the first document',\n",
    "]\n",
    "\n",
    "vocab = fit(corpus)\n",
    "print('Vocab = {}'.format(vocab)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V7eGN-QBnkdm",
    "outputId": "128c2fce-3eff-4ccc-d266-fb468737197c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 4257.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]\n",
      " [0.         0.6876236  0.         0.28108867 0.         0.53864762\n",
      "  0.28108867 0.         0.28108867]\n",
      " [0.51184851 0.         0.         0.26710379 0.51184851 0.\n",
      "  0.26710379 0.51184851 0.26710379]\n",
      " [0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def transform(corpus,vocab):\n",
    "  rows = []\n",
    "  columns = []\n",
    "  tf1=[]\n",
    "  idf1=[]\n",
    "  values = []\n",
    "  if isinstance(corpus, (list,)):   #check input\n",
    "    for idx, row in enumerate(tqdm(corpus)):\n",
    "        word_freq = dict(Counter(row.split()))\n",
    "        f=0         \n",
    "        for word, freq in word_freq.items():\n",
    "            if len(word) < 2:\n",
    "                continue    \n",
    "            f=len(row.split())    #finding requency\n",
    "            col_index = vocab.get(word, -1)[0]  #column index of output\n",
    "            if col_index !=-1:\n",
    "                rows.append(idx)        #row index of output\n",
    "                columns.append(col_index)\n",
    "                tf=freq/f\n",
    "                values.append(tf*vocab.get(word)[1])    #tf-idf value calculation      \n",
    "    return normalize(csr_matrix((values,(rows,columns)),shape=(len(corpus),len(vocab))),norm='l2')\n",
    "  else:\n",
    "    print(\"you need to pass list of strings\")\n",
    "print(transform(corpus,vocab).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This gives almost the same tfidf values as the scikit learn implementation."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment_3_Instructions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
